{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1086b40c",
   "metadata": {},
   "source": "# Phase 2: Baseline Modeling\n\n**Objective**: Establish benchmark metrics (MAE, RMSE, MAPE) using naive and statistical methods. These baselines will be used to evaluate advanced models in Phase 3.\n\n## Table of Contents\n1. Setup & Data Loading (from processed/)\n2. Train/Test Split\n3. Naive Forecasts (Naive-1, Naive-7, Moving Average)\n4. Statistical Baseline: SARIMA\n5. Cross-Validation with TimeSeriesSplit\n6. Model Comparison & Visualization\n7. Save Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd8c09",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pathlib import Path\nimport warnings\nimport logging\n\nwarnings.filterwarnings('ignore')\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Visualization config\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\nplt.rcParams['font.size'] = 11\n\n# Paths\nPROJECT_ROOT = Path(\"..\").resolve()\nPROCESSED_DIR = PROJECT_ROOT / \"Data\" / \"processed\"\nRESULTS_DIR = PROJECT_ROOT / \"results\"\nRESULTS_DIR.mkdir(exist_ok=True)\n\ndef mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate MAPE, handling zeros by filtering them out.\"\"\"\n    mask = y_true != 0\n    if mask.sum() == 0:\n        return np.nan\n    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n# Load cleaned data from Phase 1\ndata_path = PROCESSED_DIR / \"daily_sales_clean.parquet\"\nif data_path.exists():\n    df = pd.read_parquet(data_path)\n    print(f\"‚úÖ Loaded cleaned data from: {data_path}\")\nelse:\n    # Fallback to original processed data\n    data_path = PROCESSED_DIR / \"daily_sales.parquet\"\n    df = pd.read_parquet(data_path)\n    print(f\"‚ö†Ô∏è Loaded original data from: {data_path}\")\n\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\n\nprint(f\"Date range: {df.index.min().date()} to {df.index.max().date()}\")\nprint(f\"Total days: {len(df)}\")\nprint(f\"\\nData preview:\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "id": "e1aea81d",
   "metadata": {},
   "source": "## 2. Train/Test Split\n\nUsing 80/20 split, respecting time order (no shuffling)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c264bd",
   "metadata": {},
   "outputs": [],
   "source": "# Train/Test split (80/20)\ntrain_size = int(len(df) * 0.8)\ntrain = df.iloc[:train_size].copy()\ntest = df.iloc[train_size:].copy()\n\nprint(f\"Train: {train.index.min().date()} to {train.index.max().date()} ({len(train)} days)\")\nprint(f\"Test:  {test.index.min().date()} to {test.index.max().date()} ({len(test)} days)\")\n\n# Visualize split\nplt.figure(figsize=(14, 4))\nplt.plot(train.index, train['sales'], label='Train', color='blue')\nplt.plot(test.index, test['sales'], label='Test', color='orange')\nplt.axvline(x=train.index.max(), color='red', linestyle='--', label='Train/Test Split')\nplt.title(\"Train/Test Split Visualization\")\nplt.ylabel(\"Sales (PLN)\")\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "68ee5d74",
   "metadata": {},
   "source": "## 3. Naive Forecasts\n\n- **Naive-1**: Tomorrow = Today (persistence model)\n- **Naive-7**: Tomorrow = Same day last week (seasonal naive)\n- **Moving Average**: Tomorrow = Average of last 7 days"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b5919",
   "metadata": {},
   "outputs": [],
   "source": "# Store results\nresults = []\npredictions = {}\n\n# ============================================\n# NAIVE-1: Yesterday's value\n# ============================================\n# For each test day, predict using the previous day's actual value\nfull_series = pd.concat([train['sales'], test['sales']])\nnaive1_pred = full_series.shift(1).loc[test.index]\n\nnaive1_mae = mean_absolute_error(test['sales'], naive1_pred)\nnaive1_rmse = np.sqrt(mean_squared_error(test['sales'], naive1_pred))\nnaive1_mape = mean_absolute_percentage_error(test['sales'].values, naive1_pred.values)\n\nresults.append({'Model': 'Naive-1', 'MAE': naive1_mae, 'RMSE': naive1_rmse, 'MAPE': naive1_mape})\npredictions['Naive-1'] = naive1_pred\nprint(f\"Naive-1:        MAE={naive1_mae:,.0f}, RMSE={naive1_rmse:,.0f}, MAPE={naive1_mape:.1f}%\")\n\n# ============================================\n# NAIVE-7: Same day last week\n# ============================================\nnaive7_pred = full_series.shift(7).loc[test.index]\n\nnaive7_mae = mean_absolute_error(test['sales'], naive7_pred)\nnaive7_rmse = np.sqrt(mean_squared_error(test['sales'], naive7_pred))\nnaive7_mape = mean_absolute_percentage_error(test['sales'].values, naive7_pred.values)\n\nresults.append({'Model': 'Naive-7', 'MAE': naive7_mae, 'RMSE': naive7_rmse, 'MAPE': naive7_mape})\npredictions['Naive-7'] = naive7_pred\nprint(f\"Naive-7:        MAE={naive7_mae:,.0f}, RMSE={naive7_rmse:,.0f}, MAPE={naive7_mape:.1f}%\")\n\n# ============================================\n# MOVING AVERAGE (7-day rolling)\n# ============================================\n# For each test day, predict using the rolling mean of the previous 7 days\nma_pred = full_series.rolling(window=7).mean().shift(1).loc[test.index]\n\nma_mae = mean_absolute_error(test['sales'], ma_pred)\nma_rmse = np.sqrt(mean_squared_error(test['sales'], ma_pred))\nma_mape = mean_absolute_percentage_error(test['sales'].values, ma_pred.values)\n\nresults.append({'Model': 'Moving Avg (7)', 'MAE': ma_mae, 'RMSE': ma_rmse, 'MAPE': ma_mape})\npredictions['Moving Avg (7)'] = ma_pred\nprint(f\"Moving Avg (7): MAE={ma_mae:,.0f}, RMSE={ma_rmse:,.0f}, MAPE={ma_mape:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "272208cd",
   "metadata": {},
   "source": "## 4. Statistical Baseline: SARIMA\n\nSeasonal ARIMA with weekly seasonality (period=7). Using order (1,1,1) and seasonal order (1,1,1,7)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a422cd",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SARIMA (1,1,1)(1,1,1,7)\n# ============================================\nprint(\"Fitting SARIMA model... (this may take a moment)\")\n\ntry:\n    sarima_model = SARIMAX(\n        train['sales'], \n        order=(1, 1, 1), \n        seasonal_order=(1, 1, 1, 7),\n        enforce_stationarity=False,\n        enforce_invertibility=False\n    )\n    sarima_fit = sarima_model.fit(disp=False)\n    \n    # Forecast\n    sarima_pred = sarima_fit.forecast(steps=len(test))\n    sarima_pred.index = test.index\n    \n    sarima_mae = mean_absolute_error(test['sales'], sarima_pred)\n    sarima_rmse = np.sqrt(mean_squared_error(test['sales'], sarima_pred))\n    sarima_mape = mean_absolute_percentage_error(test['sales'].values, sarima_pred.values)\n    \n    results.append({'Model': 'SARIMA', 'MAE': sarima_mae, 'RMSE': sarima_rmse, 'MAPE': sarima_mape})\n    predictions['SARIMA'] = sarima_pred\n    print(f\"SARIMA:         MAE={sarima_mae:,.0f}, RMSE={sarima_rmse:,.0f}, MAPE={sarima_mape:.1f}%\")\n    \n    # Model summary\n    print(\"\\nSARIMA Model Summary:\")\n    print(f\"  AIC: {sarima_fit.aic:.0f}\")\n    print(f\"  BIC: {sarima_fit.bic:.0f}\")\n    \nexcept Exception as e:\n    print(f\"SARIMA fitting failed: {e}\")\n    sarima_pred = None"
  },
  {
   "cell_type": "markdown",
   "id": "y90jneudtum",
   "source": "## 5. Cross-Validation with TimeSeriesSplit\n\nValidating model performance across multiple time periods to ensure robustness.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jvq1vxzdibf",
   "source": "# Time Series Cross-Validation\nn_splits = 5\ntscv = TimeSeriesSplit(n_splits=n_splits)\n\ncv_results = {\n    'Naive-1': [],\n    'Naive-7': [],\n    'Moving Avg (7)': []\n}\n\nprint(f\"Running {n_splits}-fold Time Series Cross-Validation...\")\nprint(\"=\"*50)\n\nfor fold, (train_idx, val_idx) in enumerate(tscv.split(df)):\n    cv_train = df.iloc[train_idx]\n    cv_val = df.iloc[val_idx]\n    \n    # Skip if validation set is too small\n    if len(cv_val) < 7:\n        continue\n    \n    cv_full = pd.concat([cv_train['sales'], cv_val['sales']])\n    \n    # Naive-1\n    pred_naive1 = cv_full.shift(1).iloc[len(cv_train):]\n    cv_results['Naive-1'].append(mean_absolute_error(cv_val['sales'], pred_naive1))\n    \n    # Naive-7\n    pred_naive7 = cv_full.shift(7).iloc[len(cv_train):]\n    valid_mask = ~pred_naive7.isna()\n    if valid_mask.sum() > 0:\n        cv_results['Naive-7'].append(mean_absolute_error(cv_val['sales'][valid_mask], pred_naive7[valid_mask]))\n    \n    # Moving Average\n    pred_ma = cv_full.rolling(7).mean().shift(1).iloc[len(cv_train):]\n    valid_mask = ~pred_ma.isna()\n    if valid_mask.sum() > 0:\n        cv_results['Moving Avg (7)'].append(mean_absolute_error(cv_val['sales'][valid_mask], pred_ma[valid_mask]))\n    \n    print(f\"Fold {fold+1}: Train={len(cv_train)} days, Val={len(cv_val)} days\")\n\n# Summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"CROSS-VALIDATION RESULTS (MAE)\")\nprint(\"=\"*50)\ncv_summary = []\nfor model, scores in cv_results.items():\n    if scores:\n        mean_score = np.mean(scores)\n        std_score = np.std(scores)\n        cv_summary.append({\n            'Model': model,\n            'CV Mean MAE': mean_score,\n            'CV Std MAE': std_score\n        })\n        print(f\"{model:15s}: {mean_score:,.0f} ¬± {std_score:,.0f}\")\n\ncv_df = pd.DataFrame(cv_summary)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d1060e6xba7",
   "source": "## 6. Model Comparison & Visualization\n\nComparing all baseline models on the test set with comprehensive visualizations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h1zg2ku35b7",
   "source": "# Results summary table\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values('MAE')\nprint(\"=\"*60)\nprint(\"BASELINE MODEL COMPARISON (Test Set)\")\nprint(\"=\"*60)\nprint(results_df.to_string(index=False))\n\n# Identify best model\nbest_model = results_df.iloc[0]['Model']\nprint(f\"\\nüèÜ Best Baseline Model: {best_model}\")\n\n# Bar chart comparison\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# MAE\ncolors = ['green' if m == best_model else 'steelblue' for m in results_df['Model']]\naxes[0].barh(results_df['Model'], results_df['MAE'], color=colors)\naxes[0].set_xlabel('MAE (PLN)')\naxes[0].set_title('Mean Absolute Error')\naxes[0].invert_yaxis()\n\n# RMSE\naxes[1].barh(results_df['Model'], results_df['RMSE'], color=colors)\naxes[1].set_xlabel('RMSE (PLN)')\naxes[1].set_title('Root Mean Squared Error')\naxes[1].invert_yaxis()\n\n# MAPE\naxes[2].barh(results_df['Model'], results_df['MAPE'], color=colors)\naxes[2].set_xlabel('MAPE (%)')\naxes[2].set_title('Mean Absolute Percentage Error')\naxes[2].invert_yaxis()\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wzjwliz0lo",
   "source": "# Time series comparison plot - all predictions vs actual\nfig, ax = plt.subplots(figsize=(16, 6))\n\n# Actual values\nax.plot(test.index, test['sales'], 'k-', linewidth=2, label='Actual', alpha=0.8)\n\n# Predictions\ncolors = plt.cm.tab10(np.linspace(0, 1, len(predictions)))\nfor (name, pred), color in zip(predictions.items(), colors):\n    ax.plot(test.index, pred, '--', linewidth=1.5, label=name, color=color, alpha=0.7)\n\nax.set_title(\"Baseline Models: Actual vs Predictions (Test Set)\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales (PLN)\")\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Zoomed view - first 30 days of test\nfig, ax = plt.subplots(figsize=(16, 5))\nzoom_days = 30\n\nax.plot(test.index[:zoom_days], test['sales'].iloc[:zoom_days], 'ko-', linewidth=2, markersize=4, label='Actual')\nfor (name, pred), color in zip(predictions.items(), colors):\n    ax.plot(test.index[:zoom_days], pred.iloc[:zoom_days], '--', linewidth=1.5, label=name, color=color, alpha=0.7)\n\nax.set_title(f\"Zoomed View: First {zoom_days} Days of Test Set\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Sales (PLN)\")\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v0g5apl8aw",
   "source": "## 7. Save Results\n\nSaving baseline results for comparison with advanced models in Phase 3.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "30i7le839rf",
   "source": "# Save results to CSV\nresults_path = RESULTS_DIR / \"baseline_results.csv\"\nresults_df.to_csv(results_path, index=False)\nprint(f\"‚úÖ Saved baseline results to: {results_path}\")\n\n# Save predictions for later analysis\npredictions_df = pd.DataFrame(predictions)\npredictions_df['actual'] = test['sales'].values\npredictions_df['date'] = test.index\npredictions_df = predictions_df[['date', 'actual'] + list(predictions.keys())]\npredictions_path = RESULTS_DIR / \"baseline_predictions.csv\"\npredictions_df.to_csv(predictions_path, index=False)\nprint(f\"‚úÖ Saved predictions to: {predictions_path}\")\n\n# Summary for Phase 3\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY FOR PHASE 3\")\nprint(\"=\"*60)\nprint(f\"\"\"\nBest Baseline Model: {best_model}\nBest MAE: {results_df.iloc[0]['MAE']:,.0f} PLN\nBest RMSE: {results_df.iloc[0]['RMSE']:,.0f} PLN\nBest MAPE: {results_df.iloc[0]['MAPE']:.1f}%\n\nTARGET FOR ADVANCED MODELS:\n- Beat baseline MAE of {results_df.iloc[0]['MAE']:,.0f} PLN\n- Target: >10% improvement = MAE < {results_df.iloc[0]['MAE'] * 0.9:,.0f} PLN\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}