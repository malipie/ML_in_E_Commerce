{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 1: Exploratory Data Analysis & Data Cleaning\n\n**Objective**: Deeply understand the dataset, identify quality issues (missing values, outliers), and analyze time-series characteristics (seasonality, trend).\n\n## Table of Contents\n1. Setup & Configuration\n2. Data Loading (All 9 XML Files)\n3. Data Profiling & Quality Assessment\n4. Missing Values Analysis\n5. Outlier Detection & Strategy\n6. Time Series Decomposition\n7. Autocorrelation Analysis\n8. Conclusions & Cleaning Strategy\n9. Save Cleaned Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom scipy import stats\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Visualization config\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (14, 6)\nplt.rcParams[\"font.size\"] = 11\n\n# Paths\nPROJECT_ROOT = Path(\"..\").resolve()\nDATA_DIR = PROJECT_ROOT / \"Data\"\nPROCESSED_DIR = DATA_DIR / \"processed\"\nPROCESSED_DIR.mkdir(exist_ok=True)\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Processed directory: {PROCESSED_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Loading - All 9 XML Files\n\nLoading all XML files and combining them into a single DataFrame. Each file contains order data with timestamps and amounts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_single_xml(xml_path: Path) -> pd.DataFrame:\n    \"\"\"Parse a single XML file and return DataFrame with order data.\"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        data = []\n        \n        for order in root.findall('order'):\n            date_add = order.find('date_add')\n            amount = order.find('order_amount_brutto')\n            order_id = order.find('order_id')\n            \n            if date_add is not None and amount is not None:\n                date_text = date_add.text\n                amount_text = amount.text\n                order_id_text = order_id.text if order_id is not None else None\n                \n                if date_text and amount_text:\n                    data.append({\n                        'order_id': order_id_text,\n                        'date_add': date_text,\n                        'order_amount_brutto': float(amount_text),\n                        'source_file': xml_path.name\n                    })\n        \n        return pd.DataFrame(data)\n    except Exception as e:\n        logger.error(f\"Error parsing {xml_path}: {e}\")\n        return pd.DataFrame()\n\n\ndef load_all_xml_files(data_dir: Path) -> pd.DataFrame:\n    \"\"\"Load and combine all XML files from the data directory.\"\"\"\n    xml_files = sorted(data_dir.glob(\"*.xml\"))\n    logger.info(f\"Found {len(xml_files)} XML files: {[f.name for f in xml_files]}\")\n    \n    all_dfs = []\n    for xml_path in xml_files:\n        df = parse_single_xml(xml_path)\n        if not df.empty:\n            logger.info(f\"  {xml_path.name}: {len(df)} orders\")\n            all_dfs.append(df)\n    \n    if not all_dfs:\n        raise ValueError(\"No data loaded from XML files!\")\n    \n    combined_df = pd.concat(all_dfs, ignore_index=True)\n    logger.info(f\"Total orders loaded: {len(combined_df)}\")\n    return combined_df\n\n\n# Load all XML files\ndf_raw = load_all_xml_files(DATA_DIR)\n\n# Parse dates\ndf_raw['date_add'] = pd.to_datetime(df_raw['date_add'], format='%d.%m.%Y %H:%M:%S')\ndf_raw['date'] = df_raw['date_add'].dt.normalize()\n\n# Check for duplicates by order_id\nduplicates = df_raw['order_id'].duplicated().sum()\nprint(f\"\\nDuplicate order IDs: {duplicates}\")\nif duplicates > 0:\n    df_raw = df_raw.drop_duplicates(subset='order_id', keep='first')\n    print(f\"After removing duplicates: {len(df_raw)} orders\")\n\n# Display sample\nprint(f\"\\nDate range: {df_raw['date_add'].min()} to {df_raw['date_add'].max()}\")\nprint(f\"\\nSample data:\")\ndf_raw.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Profiling & Aggregation\n\nAggregating orders to daily sales level and analyzing the time series characteristics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregate to daily level\ndaily_sales = df_raw.groupby('date').agg(\n    sales=('order_amount_brutto', 'sum'),\n    order_count=('order_id', 'count'),\n    avg_order_value=('order_amount_brutto', 'mean')\n).reset_index()\n\n# Create full date range to identify missing days\nfull_range = pd.date_range(\n    start=daily_sales['date'].min(), \n    end=daily_sales['date'].max(), \n    freq='D'\n)\n\nprint(f\"Date Range: {daily_sales['date'].min().date()} to {daily_sales['date'].max().date()}\")\nprint(f\"Expected days: {len(full_range)}\")\nprint(f\"Actual days with data: {len(daily_sales)}\")\nprint(f\"Missing days: {len(full_range) - len(daily_sales)}\")\n\n# Reindex to include all days\ndf = daily_sales.set_index('date').reindex(full_range).rename_axis('date').reset_index()\n\n# Statistical summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"STATISTICAL SUMMARY\")\nprint(\"=\"*50)\nprint(df[['sales', 'order_count', 'avg_order_value']].describe())\n\n# Plot daily sales\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\naxes[0].plot(df['date'], df['sales'], linewidth=0.8)\naxes[0].set_title(\"Daily Sales Volume (PLN)\")\naxes[0].set_ylabel(\"Sales\")\naxes[0].axhline(y=df['sales'].mean(), color='r', linestyle='--', label=f\"Mean: {df['sales'].mean():.0f}\")\naxes[0].legend()\n\naxes[1].plot(df['date'], df['order_count'], linewidth=0.8, color='green')\naxes[1].set_title(\"Daily Order Count\")\naxes[1].set_ylabel(\"Orders\")\naxes[1].axhline(y=df['order_count'].mean(), color='r', linestyle='--', label=f\"Mean: {df['order_count'].mean():.0f}\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Missing Values Analysis\n\nIdentifying gaps in the time series and deciding on imputation strategy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify missing days\nmissing_mask = df['sales'].isna()\nmissing_days = df[missing_mask]['date'].tolist()\n\nprint(f\"Total missing days: {len(missing_days)}\")\nif missing_days:\n    print(f\"\\nMissing dates:\")\n    for d in missing_days[:20]:  # Show first 20\n        print(f\"  {d.strftime('%Y-%m-%d')} ({d.strftime('%A')})\")\n    if len(missing_days) > 20:\n        print(f\"  ... and {len(missing_days) - 20} more\")\n\n# Analyze missing days by weekday\nif missing_days:\n    missing_weekdays = pd.Series([d.dayofweek for d in missing_days]).value_counts().sort_index()\n    weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n    missing_weekdays.index = [weekday_names[i] for i in missing_weekdays.index]\n    \n    plt.figure(figsize=(10, 4))\n    missing_weekdays.plot(kind='bar', color='coral')\n    plt.title(\"Missing Days by Weekday\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=0)\n    plt.show()\n\n# Visualize missing data in time series\nplt.figure(figsize=(14, 4))\nplt.plot(df['date'], df['sales'], linewidth=0.8, label='Available data')\nif missing_days:\n    plt.scatter(missing_days, [0]*len(missing_days), color='red', s=20, label='Missing days', zorder=5)\nplt.title(\"Missing Data Visualization\")\nplt.ylabel(\"Sales\")\nplt.legend()\nplt.show()\n\n# Imputation Strategy Decision\nprint(\"\\n\" + \"=\"*50)\nprint(\"IMPUTATION STRATEGY\")\nprint(\"=\"*50)\nprint(\"\"\"\nFor e-commerce time series, missing days likely mean:\n1. No sales on that day (real zero) - common for small businesses\n2. Data collection issue - less likely with automated systems\n\nDECISION: Fill missing days with 0 (assuming no sales)\nRATIONALE: E-commerce platforms typically don't skip days in logs,\n           so missing data = no transactions occurred.\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Outlier Detection & Strategy\n\nIdentifying extreme values using statistical methods (IQR, Z-score) and deciding on handling approach."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fill missing values first for outlier analysis\ndf['sales'] = df['sales'].fillna(0)\ndf['order_count'] = df['order_count'].fillna(0)\ndf['avg_order_value'] = df['avg_order_value'].fillna(0)\n\n# Only analyze non-zero days for outliers (zero days are valid)\nsales_nonzero = df[df['sales'] > 0]['sales']\n\n# Method 1: IQR\nQ1 = sales_nonzero.quantile(0.25)\nQ3 = sales_nonzero.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Method 2: Z-score\nz_scores = np.abs(stats.zscore(sales_nonzero))\nz_threshold = 3\n\nprint(\"=\"*50)\nprint(\"OUTLIER DETECTION RESULTS\")\nprint(\"=\"*50)\nprint(f\"\\nIQR Method:\")\nprint(f\"  Q1: {Q1:,.0f}, Q3: {Q3:,.0f}, IQR: {IQR:,.0f}\")\nprint(f\"  Lower bound: {lower_bound:,.0f}\")\nprint(f\"  Upper bound: {upper_bound:,.0f}\")\n\n# Identify outliers and FLAG them in the dataframe\ndf['is_outlier'] = ((df['sales'] > upper_bound) | ((df['sales'] < lower_bound) & (df['sales'] > 0))).astype(int)\niqr_outliers = df[df['is_outlier'] == 1]\nprint(f\"  Outliers found (IQR): {len(iqr_outliers)}\")\n\n# Z-score outliers\ndf_nonzero = df[df['sales'] > 0].copy()\ndf_nonzero['z_score'] = np.abs(stats.zscore(df_nonzero['sales']))\nz_outliers = df_nonzero[df_nonzero['z_score'] > z_threshold]\nprint(f\"\\nZ-score Method (threshold={z_threshold}):\")\nprint(f\"  Outliers found (Z-score): {len(z_outliers)}\")\n\n# Visualize outliers\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Distribution with outlier bounds\naxes[0].hist(sales_nonzero, bins=50, edgecolor='black', alpha=0.7)\naxes[0].axvline(upper_bound, color='red', linestyle='--', label=f'Upper IQR: {upper_bound:,.0f}')\naxes[0].axvline(lower_bound, color='orange', linestyle='--', label=f'Lower IQR: {lower_bound:,.0f}')\naxes[0].set_title(\"Sales Distribution with IQR Bounds\")\naxes[0].set_xlabel(\"Sales\")\naxes[0].legend()\n\n# Boxplot\naxes[1].boxplot(sales_nonzero, vert=False)\naxes[1].set_title(\"Sales Boxplot (Non-zero days)\")\naxes[1].set_xlabel(\"Sales\")\n\nplt.tight_layout()\nplt.show()\n\n# Show outlier days\nif len(iqr_outliers) > 0:\n    print(\"\\nTop outlier days (highest sales):\")\n    top_outliers = iqr_outliers.nlargest(10, 'sales')[['date', 'sales', 'order_count']]\n    for _, row in top_outliers.iterrows():\n        print(f\"  {row['date'].strftime('%Y-%m-%d')} ({row['date'].strftime('%A')}): {row['sales']:,.0f} PLN, {int(row['order_count'])} orders\")\n\nprint(f\"\\n‚úÖ Created 'is_outlier' column in dataframe\")\n\n# OUTLIER TREATMENT STRATEGY: Replace outliers with rolling median\nprint(\"\\n\" + \"=\"*50)\nprint(\"OUTLIER TREATMENT\")\nprint(\"=\"*50)\n\n# Store original values for comparison\ndf['sales_original'] = df['sales'].copy()\n\n# Calculate rolling median (window=7 for weekly pattern)\ndf['rolling_median_7'] = df['sales'].rolling(window=7, center=True, min_periods=1).median()\n\n# Replace outliers with rolling median\noutlier_count = df['is_outlier'].sum()\ndf.loc[df['is_outlier'] == 1, 'sales'] = df.loc[df['is_outlier'] == 1, 'rolling_median_7']\n\nprint(f\"Outliers replaced: {outlier_count}\")\nprint(f\"Strategy: Replace with 7-day rolling median (centered)\")\nprint(f\"\\nBefore vs After treatment:\")\ncomparison = df[df['is_outlier'] == 1][['date', 'sales_original', 'sales', 'rolling_median_7']].head(10)\ncomparison.columns = ['Date', 'Original', 'Replaced', 'Rolling Median']\nfor _, row in comparison.iterrows():\n    print(f\"  {row['Date'].strftime('%Y-%m-%d')}: {row['Original']:,.0f} ‚Üí {row['Replaced']:,.0f} PLN\")\n\n# Visualize before/after\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\naxes[0].plot(df['date'], df['sales_original'], linewidth=0.8, alpha=0.7, label='Original')\naxes[0].scatter(df[df['is_outlier']==1]['date'], df[df['is_outlier']==1]['sales_original'],\n                color='red', s=50, label='Outliers', zorder=5)\naxes[0].set_title(\"Original Sales with Outliers Highlighted\")\naxes[0].set_ylabel(\"Sales (PLN)\")\naxes[0].legend()\n\naxes[1].plot(df['date'], df['sales'], linewidth=0.8, color='green', label='Cleaned (outliers replaced)')\naxes[1].set_title(\"Sales After Outlier Treatment (Rolling Median Replacement)\")\naxes[1].set_ylabel(\"Sales (PLN)\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n‚úÖ Outliers treated - replaced with rolling median values\")"
  },
  {
   "cell_type": "markdown",
   "id": "ls88feqms6h",
   "source": "## 6. Time Series Decomposition\n\nDecomposing the time series into Trend, Seasonality, and Residuals to understand underlying patterns.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Prepare data for decomposition (need datetime index)\ndf_ts = df.set_index('date')['sales']\n\n# Weekly seasonality (period=7)\ndecomp_weekly = seasonal_decompose(df_ts, model='additive', period=7)\n\nfig, axes = plt.subplots(4, 1, figsize=(14, 12))\ndecomp_weekly.observed.plot(ax=axes[0], title='Observed')\ndecomp_weekly.trend.plot(ax=axes[1], title='Trend')\ndecomp_weekly.seasonal.plot(ax=axes[2], title='Weekly Seasonality (period=7)')\ndecomp_weekly.resid.plot(ax=axes[3], title='Residuals')\nplt.tight_layout()\nplt.show()\n\n# Check for monthly seasonality if enough data\nif len(df_ts) > 60:  # At least 2 months\n    decomp_monthly = seasonal_decompose(df_ts, model='additive', period=30)\n    \n    fig, ax = plt.subplots(figsize=(14, 3))\n    decomp_monthly.seasonal.plot(ax=ax, title='Monthly Seasonality (period=30)')\n    plt.tight_layout()\n    plt.show()\n\n# Weekday analysis\ndf['weekday'] = df['date'].dt.dayofweek\nweekday_sales = df.groupby('weekday')['sales'].agg(['mean', 'median', 'std'])\nweekday_sales.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n\nprint(\"\\nSales by Weekday:\")\nprint(weekday_sales.round(0))\n\nfig, ax = plt.subplots(figsize=(10, 5))\nweekday_sales['mean'].plot(kind='bar', ax=ax, color='steelblue', alpha=0.7, label='Mean')\nweekday_sales['median'].plot(kind='bar', ax=ax, color='coral', alpha=0.5, label='Median')\nax.set_title(\"Average Sales by Weekday\")\nax.set_ylabel(\"Sales (PLN)\")\nax.set_xticklabels(weekday_sales.index, rotation=0)\nax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "fowoqsqmzho",
   "source": "## 7. Autocorrelation Analysis\n\nExamining ACF and PACF to identify lag dependencies and inform model selection (ARIMA orders).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fvyp36dzvy",
   "source": "# ACF and PACF plots\n# Dynamically set max lags based on data size (max 50% of sample)\nmax_lags = min(35, len(df_ts) // 2 - 1)\nprint(f\"Using {max_lags} lags (data has {len(df_ts)} observations)\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\nplot_acf(df_ts, lags=max_lags, ax=axes[0])\naxes[0].set_title(\"Autocorrelation Function (ACF)\")\nif max_lags >= 7:\n    axes[0].axvline(x=7, color='red', linestyle='--', alpha=0.5, label='Lag 7 (weekly)')\nif max_lags >= 14:\n    axes[0].axvline(x=14, color='red', linestyle='--', alpha=0.5)\n\nplot_pacf(df_ts, lags=max_lags, ax=axes[1], method='ywm')\naxes[1].set_title(\"Partial Autocorrelation Function (PACF)\")\nif max_lags >= 7:\n    axes[1].axvline(x=7, color='red', linestyle='--', alpha=0.5, label='Lag 7 (weekly)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\"\"\nINTERPRETATION:\n- ACF: Significant spikes at lags 7, 14, 21, 28 confirm WEEKLY SEASONALITY\n- PACF: Sharp cutoff suggests AR component is appropriate\n- Recommendation: SARIMA with seasonal period=7 or models with weekly lag features\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "oeuk818ctj",
   "source": "## 8. Conclusions & Cleaning Strategy\n\nSummary of findings and decisions for data preprocessing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m6lonhe7ik8",
   "source": "# Generate summary report\nprint(\"=\"*60)\nprint(\"EDA SUMMARY REPORT\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nüìÖ DATE RANGE\n   Start: {df['date'].min().strftime('%Y-%m-%d')}\n   End:   {df['date'].max().strftime('%Y-%m-%d')}\n   Total days: {len(df)}\n\nüìä DATA QUALITY\n   Missing days: {len(missing_days)} ({100*len(missing_days)/len(full_range):.1f}%)\n   Strategy: Fill with 0 (no sales)\n   \n   Outliers detected (IQR): {df['is_outlier'].sum()} ({100*df['is_outlier'].mean():.1f}%)\n   Strategy: REPLACED with 7-day rolling median (treated as anomalies)\n\nüìà SALES STATISTICS (after outlier treatment)\n   Mean daily sales: {df['sales'].mean():,.0f} PLN\n   Median daily sales: {df['sales'].median():,.0f} PLN\n   Std deviation: {df['sales'].std():,.0f} PLN\n   Min: {df['sales'].min():,.0f} PLN\n   Max: {df['sales'].max():,.0f} PLN\n   \n   Mean orders/day: {df['order_count'].mean():.1f}\n   Mean order value: {df[df['avg_order_value'] > 0]['avg_order_value'].mean():.0f} PLN\n\nüîÑ SEASONALITY\n   Weekly pattern: CONFIRMED (ACF significant at lags 7, 14, 21, 28)\n   Best sales day: {weekday_sales['mean'].idxmax()}\n   Worst sales day: {weekday_sales['mean'].idxmin()}\n   \nüìù RECOMMENDATIONS FOR MODELING\n   1. Use SARIMA with seasonal period=7\n   2. Include lag features: lag_1, lag_7, lag_14\n   3. Include calendar features: day_of_week, is_weekend\n   4. Consider rolling statistics: rolling_mean_7, rolling_std_7\n   5. Outliers already normalized - no need for additional treatment\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4gzglkg0mku",
   "source": "## 9. Save Cleaned Data\n\nSaving the processed daily sales data for use in modeling notebooks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ftt1g0sob66",
   "source": "# Prepare final cleaned dataset\ndf_clean = df[['date', 'sales', 'sales_original', 'order_count', 'avg_order_value', 'is_outlier', 'weekday']].copy()\n\n# Add useful features for modeling\ndf_clean['is_weekend'] = df_clean['weekday'].isin([5, 6]).astype(int)\ndf_clean['month'] = df_clean['date'].dt.month\ndf_clean['day_of_month'] = df_clean['date'].dt.day\ndf_clean['week_of_year'] = df_clean['date'].dt.isocalendar().week.astype(int)\n\n# Save to parquet\noutput_path = PROCESSED_DIR / \"daily_sales_clean.parquet\"\ndf_clean.to_parquet(output_path, index=False)\nprint(f\"‚úÖ Saved cleaned data to: {output_path}\")\nprint(f\"   Shape: {df_clean.shape}\")\nprint(f\"   Columns: {list(df_clean.columns)}\")\nprint(f\"\\n   Note: 'sales' contains normalized values (outliers replaced)\")\nprint(f\"         'sales_original' contains original values for reference\")\n\n# Also save raw orders for potential future use\ndf_raw.to_parquet(PROCESSED_DIR / \"orders_raw.parquet\", index=False)\nprint(f\"‚úÖ Saved raw orders to: {PROCESSED_DIR / 'orders_raw.parquet'}\")\n\n# Preview final data\nprint(\"\\nFinal cleaned data preview:\")\ndf_clean.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}