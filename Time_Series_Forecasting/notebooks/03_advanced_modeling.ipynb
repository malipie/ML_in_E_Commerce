{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0713b6d",
   "metadata": {},
   "source": "# Phase 3: Advanced Modeling & Experimentation\n\n**Objective**: Beat the baseline with ML models, feature engineering, and hyperparameter tuning. Includes anomaly detection and model interpretability.\n\n## Table of Contents\n1. Setup & Data Loading\n2. Anomaly Detection (Isolation Forest, Z-score)\n3. Feature Engineering\n4. Model Training (XGBoost, Prophet)\n5. Hyperparameter Optimization (Optuna)\n6. Feature Importance Analysis\n7. Residual Analysis\n8. Model Comparison (vs Baseline)\n9. MLflow Logging & Champion Model Selection\n10. Save Champion Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbb01a",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import IsolationForest\nfrom scipy import stats\nimport xgboost as xgb\nfrom prophet import Prophet\nimport optuna\nimport mlflow\nimport mlflow.sklearn\nimport joblib\nfrom pathlib import Path\nimport warnings\nimport logging\n\nwarnings.filterwarnings('ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Visualization config\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\nplt.rcParams['font.size'] = 11\n\n# Paths\nPROJECT_ROOT = Path(\"..\").resolve()\nPROCESSED_DIR = PROJECT_ROOT / \"Data\" / \"processed\"\nRESULTS_DIR = PROJECT_ROOT / \"results\"\nMODELS_DIR = PROJECT_ROOT / \"models\"\nRESULTS_DIR.mkdir(exist_ok=True)\nMODELS_DIR.mkdir(exist_ok=True)\n\ndef mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculate MAPE, handling zeros.\"\"\"\n    mask = y_true != 0\n    if mask.sum() == 0:\n        return np.nan\n    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n# Load cleaned data\ndata_path = PROCESSED_DIR / \"daily_sales_clean.parquet\"\nif data_path.exists():\n    df = pd.read_parquet(data_path)\n    print(f\"‚úÖ Loaded cleaned data from: {data_path}\")\nelse:\n    data_path = PROCESSED_DIR / \"daily_sales.parquet\"\n    df = pd.read_parquet(data_path)\n    print(f\"‚ö†Ô∏è Loaded original data from: {data_path}\")\n\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\n\n# Load baseline results for comparison\nbaseline_path = RESULTS_DIR / \"baseline_results.csv\"\nif baseline_path.exists():\n    baseline_results = pd.read_csv(baseline_path)\n    best_baseline_mae = baseline_results['MAE'].min()\n    print(f\"üìä Best baseline MAE: {best_baseline_mae:,.0f} PLN\")\nelse:\n    best_baseline_mae = None\n    print(\"‚ö†Ô∏è No baseline results found\")\n\nprint(f\"\\nData shape: {df.shape}\")\nprint(f\"Date range: {df.index.min().date()} to {df.index.max().date()}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "id": "1c3c87c4",
   "metadata": {},
   "source": "## 2. Anomaly Detection\n\nDetecting anomalies using multiple methods:\n- **Isolation Forest**: Unsupervised ML approach\n- **Z-score**: Statistical threshold method\n- **IQR**: Interquartile range method"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968121de",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# ANOMALY DETECTION\n# ============================================\n\n# Only analyze non-zero sales for anomaly detection\nsales_data = df[df['sales'] > 0]['sales'].values.reshape(-1, 1)\n\n# Method 1: Isolation Forest\niso_forest = IsolationForest(contamination=0.05, random_state=42, n_estimators=100)\ndf['anomaly_isoforest'] = 0\ndf.loc[df['sales'] > 0, 'anomaly_isoforest'] = iso_forest.fit_predict(sales_data)\ndf['anomaly_isoforest'] = (df['anomaly_isoforest'] == -1).astype(int)\n\n# Method 2: Z-score (threshold = 3)\nz_scores = np.zeros(len(df))\nnonzero_mask = df['sales'] > 0\nz_scores[nonzero_mask] = np.abs(stats.zscore(df.loc[nonzero_mask, 'sales']))\ndf['z_score'] = z_scores\ndf['anomaly_zscore'] = (df['z_score'] > 3).astype(int)\n\n# Method 3: IQR\nQ1 = df.loc[nonzero_mask, 'sales'].quantile(0.25)\nQ3 = df.loc[nonzero_mask, 'sales'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\ndf['anomaly_iqr'] = ((df['sales'] > upper_bound) | ((df['sales'] < lower_bound) & (df['sales'] > 0))).astype(int)\n\n# Combined anomaly flag (any method flags it)\ndf['anomaly_any'] = ((df['anomaly_isoforest'] == 1) | (df['anomaly_zscore'] == 1) | (df['anomaly_iqr'] == 1)).astype(int)\n\n# Summary\nprint(\"=\"*50)\nprint(\"ANOMALY DETECTION RESULTS\")\nprint(\"=\"*50)\nprint(f\"\\nIsolation Forest: {df['anomaly_isoforest'].sum()} anomalies ({100*df['anomaly_isoforest'].mean():.1f}%)\")\nprint(f\"Z-score (>3):     {df['anomaly_zscore'].sum()} anomalies ({100*df['anomaly_zscore'].mean():.1f}%)\")\nprint(f\"IQR method:       {df['anomaly_iqr'].sum()} anomalies ({100*df['anomaly_iqr'].mean():.1f}%)\")\nprint(f\"Combined (any):   {df['anomaly_any'].sum()} anomalies ({100*df['anomaly_any'].mean():.1f}%)\")\n\n# Visualize anomalies\nfig, axes = plt.subplots(2, 1, figsize=(16, 8))\n\n# Full time series with anomalies\naxes[0].plot(df.index, df['sales'], 'b-', linewidth=0.8, alpha=0.7, label='Sales')\nanomaly_dates = df[df['anomaly_any'] == 1].index\nanomaly_values = df.loc[anomaly_dates, 'sales']\naxes[0].scatter(anomaly_dates, anomaly_values, c='red', s=50, label='Anomalies', zorder=5)\naxes[0].axhline(y=upper_bound, color='orange', linestyle='--', alpha=0.5, label=f'IQR Upper: {upper_bound:,.0f}')\naxes[0].set_title(\"Daily Sales with Detected Anomalies\")\naxes[0].set_ylabel(\"Sales (PLN)\")\naxes[0].legend()\n\n# Anomaly score distribution (Isolation Forest)\niso_scores = iso_forest.decision_function(sales_data)\naxes[1].hist(iso_scores, bins=50, edgecolor='black', alpha=0.7)\naxes[1].axvline(x=0, color='red', linestyle='--', label='Anomaly threshold')\naxes[1].set_title(\"Isolation Forest Anomaly Scores\")\naxes[1].set_xlabel(\"Anomaly Score (negative = more anomalous)\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Show top anomalies\nprint(\"\\nTop 10 Anomalous Days (by sales value):\")\ntop_anomalies = df[df['anomaly_any'] == 1].nlargest(10, 'sales')[['sales', 'anomaly_isoforest', 'anomaly_zscore', 'anomaly_iqr']]\nfor idx, row in top_anomalies.iterrows():\n    methods = []\n    if row['anomaly_isoforest']: methods.append('IsoForest')\n    if row['anomaly_zscore']: methods.append('Z-score')\n    if row['anomaly_iqr']: methods.append('IQR')\n    print(f\"  {idx.strftime('%Y-%m-%d')} ({idx.strftime('%A'):9s}): {row['sales']:>10,.0f} PLN - {', '.join(methods)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "q9s51mo7i2j",
   "source": "## 3. Feature Engineering\n\nCreating features for ML models:\n- Calendar features (day of week, month, cyclic encoding)\n- Lag features (lag 1, 7, 14)\n- Rolling statistics (mean, std)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5aat17lbup",
   "source": "# ============================================\n# FEATURE ENGINEERING\n# ============================================\n\n# Calendar features\nif 'weekday' not in df.columns:\n    df['weekday'] = df.index.dayofweek\ndf['month'] = df.index.month\ndf['day_of_year'] = df.index.dayofyear\ndf['week_of_year'] = df.index.isocalendar().week.astype(int)\ndf['is_weekend'] = (df['weekday'] >= 5).astype(int)\n\n# Cyclic encoding for periodicity\ndf['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\ndf['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\ndf['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\ndf['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\ndf['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\ndf['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n\n# Lag features\nfor lag in [1, 7, 14, 21, 28]:\n    df[f'lag_{lag}'] = df['sales'].shift(lag)\n\n# Rolling statistics\nfor window in [7, 14, 28]:\n    df[f'rolling_mean_{window}'] = df['sales'].shift(1).rolling(window=window).mean()\n    df[f'rolling_std_{window}'] = df['sales'].shift(1).rolling(window=window).std()\n    df[f'rolling_min_{window}'] = df['sales'].shift(1).rolling(window=window).min()\n    df[f'rolling_max_{window}'] = df['sales'].shift(1).rolling(window=window).max()\n\n# Expanding mean (all historical data)\ndf['expanding_mean'] = df['sales'].shift(1).expanding().mean()\n\n# Drop rows with NaN from lag/rolling features\ndf_model = df.dropna().copy()\n\nprint(f\"Features created: {df_model.shape[1]} columns\")\nprint(f\"Data points available for modeling: {len(df_model)}\")\nprint(f\"\\nFeature list:\")\nfeature_cols = [c for c in df_model.columns if c not in ['sales', 'order_count', 'avg_order_value', \n                                                          'anomaly_isoforest', 'anomaly_zscore', \n                                                          'anomaly_iqr', 'anomaly_any', 'z_score', 'is_outlier']]\nfor i, col in enumerate(feature_cols, 1):\n    print(f\"  {i:2d}. {col}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dac65140",
   "metadata": {},
   "source": "## 4. Model Training\n\nTraining XGBoost and Prophet models on the prepared data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b89181",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRAIN/TEST SPLIT\n# ============================================\ntrain_size = int(len(df_model) * 0.8)\ntrain = df_model.iloc[:train_size]\ntest = df_model.iloc[train_size:]\n\n# Define features (exclude target and metadata columns)\nexclude_cols = ['sales', 'order_count', 'avg_order_value', 'anomaly_isoforest', \n                'anomaly_zscore', 'anomaly_iqr', 'anomaly_any', 'z_score', 'is_outlier']\nfeatures = [c for c in df_model.columns if c not in exclude_cols]\n\nX_train = train[features]\ny_train = train['sales']\nX_test = test[features]\ny_test = test['sales']\n\nprint(f\"Train: {len(train)} samples\")\nprint(f\"Test:  {len(test)} samples\")\nprint(f\"Features: {len(features)}\")\n\n# Store results\nresults = []\npredictions = {}\n\n# ============================================\n# XGBOOST (default params)\n# ============================================\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training XGBoost (default params)...\")\nprint(\"=\"*50)\n\nxgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    random_state=42\n)\nxgb_model.fit(X_train, y_train)\nxgb_pred = xgb_model.predict(X_test)\n\nxgb_mae = mean_absolute_error(y_test, xgb_pred)\nxgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\nxgb_mape = mean_absolute_percentage_error(y_test.values, xgb_pred)\n\nresults.append({'Model': 'XGBoost (default)', 'MAE': xgb_mae, 'RMSE': xgb_rmse, 'MAPE': xgb_mape})\npredictions['XGBoost (default)'] = xgb_pred\nprint(f\"XGBoost: MAE={xgb_mae:,.0f}, RMSE={xgb_rmse:,.0f}, MAPE={xgb_mape:.1f}%\")\n\n# ============================================\n# PROPHET\n# ============================================\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training Prophet...\")\nprint(\"=\"*50)\n\n# Prepare data for Prophet\nprophet_train = train.reset_index()[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\nprophet_test = test.reset_index()[['date', 'sales']].rename(columns={'date': 'ds', 'sales': 'y'})\n\nprophet_model = Prophet(\n    yearly_seasonality=True,\n    weekly_seasonality=True,\n    daily_seasonality=False\n)\nprophet_model.fit(prophet_train)\n\nfuture = prophet_model.make_future_dataframe(periods=len(prophet_test))\nforecast = prophet_model.predict(future)\nprophet_pred = forecast['yhat'].iloc[-len(prophet_test):].values\n\nprophet_mae = mean_absolute_error(prophet_test['y'], prophet_pred)\nprophet_rmse = np.sqrt(mean_squared_error(prophet_test['y'], prophet_pred))\nprophet_mape = mean_absolute_percentage_error(prophet_test['y'].values, prophet_pred)\n\nresults.append({'Model': 'Prophet', 'MAE': prophet_mae, 'RMSE': prophet_rmse, 'MAPE': prophet_mape})\npredictions['Prophet'] = prophet_pred\nprint(f\"Prophet: MAE={prophet_mae:,.0f}, RMSE={prophet_rmse:,.0f}, MAPE={prophet_mape:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "c7de2bfd",
   "metadata": {},
   "source": "## 5. Hyperparameter Optimization with Optuna\n\nTuning XGBoost hyperparameters using Bayesian optimization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee552e",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# OPTUNA HYPERPARAMETER OPTIMIZATION\n# ============================================\nprint(\"=\"*50)\nprint(\"Running Optuna optimization (20 trials)...\")\nprint(\"=\"*50)\n\ndef objective(trial):\n    \"\"\"Optuna objective function for XGBoost tuning.\"\"\"\n    params = {\n        'objective': 'reg:squarederror',\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'random_state': 42\n    }\n    \n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    return mean_absolute_error(y_test, pred)\n\n# Run optimization\nstudy = optuna.create_study(direction='minimize', study_name='xgboost_tuning')\nstudy.optimize(objective, n_trials=20, show_progress_bar=True)\n\nprint(f\"\\nBest MAE: {study.best_value:,.0f}\")\nprint(f\"Best params: {study.best_params}\")\n\n# Train final model with best params\nbest_params = study.best_params\nbest_params['objective'] = 'reg:squarederror'\nbest_params['random_state'] = 42\n\nbest_xgb = xgb.XGBRegressor(**best_params)\nbest_xgb.fit(X_train, y_train)\nbest_xgb_pred = best_xgb.predict(X_test)\n\nbest_mae = mean_absolute_error(y_test, best_xgb_pred)\nbest_rmse = np.sqrt(mean_squared_error(y_test, best_xgb_pred))\nbest_mape = mean_absolute_percentage_error(y_test.values, best_xgb_pred)\n\nresults.append({'Model': 'XGBoost (tuned)', 'MAE': best_mae, 'RMSE': best_rmse, 'MAPE': best_mape})\npredictions['XGBoost (tuned)'] = best_xgb_pred\nprint(f\"\\nXGBoost (tuned): MAE={best_mae:,.0f}, RMSE={best_rmse:,.0f}, MAPE={best_mape:.1f}%\")\n\n# Optuna visualization - these functions create their own figures\n# Optimization history\nfig_history = optuna.visualization.matplotlib.plot_optimization_history(study)\nfig_history.figure.set_size_inches(14, 4)\nplt.title(\"Optuna Optimization History\")\nplt.tight_layout()\nplt.show()\n\n# Parameter importance\ntry:\n    fig_importance = optuna.visualization.matplotlib.plot_param_importances(study)\n    fig_importance.figure.set_size_inches(14, 4)\n    plt.title(\"Hyperparameter Importance\")\n    plt.tight_layout()\n    plt.show()\nexcept Exception as e:\n    print(f\"Could not plot parameter importance: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "221df248",
   "metadata": {},
   "source": "## 6. Feature Importance Analysis\n\nUnderstanding which features contribute most to the XGBoost predictions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3757b8",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# FEATURE IMPORTANCE\n# ============================================\n\n# Get feature importance from tuned XGBoost\nimportance_df = pd.DataFrame({\n    'feature': features,\n    'importance': best_xgb.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"=\"*50)\nprint(\"TOP 15 MOST IMPORTANT FEATURES\")\nprint(\"=\"*50)\nprint(importance_df.head(15).to_string(index=False))\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Bar chart - top 15\ntop_features = importance_df.head(15)\naxes[0].barh(range(len(top_features)), top_features['importance'].values, color='steelblue')\naxes[0].set_yticks(range(len(top_features)))\naxes[0].set_yticklabels(top_features['feature'].values)\naxes[0].invert_yaxis()\naxes[0].set_xlabel('Importance')\naxes[0].set_title('Top 15 Feature Importance (XGBoost)')\n\n# Cumulative importance\nimportance_df['cumulative'] = importance_df['importance'].cumsum() / importance_df['importance'].sum()\naxes[1].plot(range(len(importance_df)), importance_df['cumulative'].values, 'b-', linewidth=2)\naxes[1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\naxes[1].axhline(y=0.95, color='orange', linestyle='--', label='95% threshold')\naxes[1].set_xlabel('Number of Features')\naxes[1].set_ylabel('Cumulative Importance')\naxes[1].set_title('Cumulative Feature Importance')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Find features that explain 80% and 95% of importance\nn_80 = (importance_df['cumulative'] <= 0.8).sum() + 1\nn_95 = (importance_df['cumulative'] <= 0.95).sum() + 1\nprint(f\"\\nFeatures needed for 80% importance: {n_80}\")\nprint(f\"Features needed for 95% importance: {n_95}\")"
  },
  {
   "cell_type": "markdown",
   "id": "rweoga1p1ec",
   "source": "## 7. Residual Analysis\n\nExamining prediction errors to understand model behavior and identify potential improvements.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rwt8abt2az",
   "source": "# ============================================\n# RESIDUAL ANALYSIS (XGBoost Tuned)\n# ============================================\n\nresiduals = y_test.values - best_xgb_pred\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Residuals over time\naxes[0, 0].plot(test.index, residuals, 'b-', linewidth=0.8)\naxes[0, 0].axhline(y=0, color='red', linestyle='--')\naxes[0, 0].fill_between(test.index, residuals, 0, alpha=0.3)\naxes[0, 0].set_title('Residuals Over Time')\naxes[0, 0].set_xlabel('Date')\naxes[0, 0].set_ylabel('Residual (Actual - Predicted)')\n\n# 2. Residual distribution\naxes[0, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\naxes[0, 1].axvline(x=0, color='red', linestyle='--')\naxes[0, 1].axvline(x=np.mean(residuals), color='green', linestyle='--', label=f'Mean: {np.mean(residuals):,.0f}')\naxes[0, 1].set_title('Residual Distribution')\naxes[0, 1].set_xlabel('Residual')\naxes[0, 1].legend()\n\n# 3. Actual vs Predicted\naxes[1, 0].scatter(y_test, best_xgb_pred, alpha=0.5, s=20)\nmax_val = max(y_test.max(), best_xgb_pred.max())\naxes[1, 0].plot([0, max_val], [0, max_val], 'r--', label='Perfect prediction')\naxes[1, 0].set_title('Actual vs Predicted')\naxes[1, 0].set_xlabel('Actual Sales')\naxes[1, 0].set_ylabel('Predicted Sales')\naxes[1, 0].legend()\n\n# 4. Residuals vs Predicted (check for heteroscedasticity)\naxes[1, 1].scatter(best_xgb_pred, residuals, alpha=0.5, s=20)\naxes[1, 1].axhline(y=0, color='red', linestyle='--')\naxes[1, 1].set_title('Residuals vs Predicted (Heteroscedasticity Check)')\naxes[1, 1].set_xlabel('Predicted Sales')\naxes[1, 1].set_ylabel('Residual')\n\nplt.tight_layout()\nplt.show()\n\n# Residual statistics\nprint(\"=\"*50)\nprint(\"RESIDUAL STATISTICS\")\nprint(\"=\"*50)\nprint(f\"Mean residual: {np.mean(residuals):,.0f} (should be ~0)\")\nprint(f\"Std residual:  {np.std(residuals):,.0f}\")\nprint(f\"Min residual:  {np.min(residuals):,.0f}\")\nprint(f\"Max residual:  {np.max(residuals):,.0f}\")\n\n# Check for autocorrelation in residuals\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nlb_test = acorr_ljungbox(residuals, lags=[7, 14], return_df=True)\nprint(f\"\\nLjung-Box test for autocorrelation:\")\nprint(lb_test)\nprint(\"\\n(p-value > 0.05 suggests no significant autocorrelation)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6um87ks7jsh",
   "source": "## 8. Model Comparison (vs Baseline)\n\nComparing advanced models against the baseline models from Phase 2.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1xw87xfzvpm",
   "source": "# ============================================\n# MODEL COMPARISON\n# ============================================\n\n# Advanced model results\nresults_df = pd.DataFrame(results)\n\n# Load baseline results if available\nif baseline_path.exists():\n    baseline_df = pd.read_csv(baseline_path)\n    baseline_df['Type'] = 'Baseline'\n    results_df['Type'] = 'Advanced'\n    all_results = pd.concat([baseline_df, results_df], ignore_index=True)\nelse:\n    results_df['Type'] = 'Advanced'\n    all_results = results_df.copy()\n\n# Sort by MAE\nall_results = all_results.sort_values('MAE')\n\nprint(\"=\"*60)\nprint(\"COMPLETE MODEL COMPARISON\")\nprint(\"=\"*60)\nprint(all_results.to_string(index=False))\n\n# Identify champion\nchampion_model_name = all_results.iloc[0]['Model']\nchampion_mae = all_results.iloc[0]['MAE']\nchampion_type = all_results.iloc[0]['Type']\n\nprint(f\"\\nüèÜ CHAMPION MODEL: {champion_model_name}\")\nprint(f\"   MAE: {champion_mae:,.0f} PLN\")\nprint(f\"   Type: {champion_type}\")\n\n# Improvement over baseline\nif best_baseline_mae:\n    improvement = (best_baseline_mae - champion_mae) / best_baseline_mae * 100\n    print(f\"\\nüìà Improvement over best baseline: {improvement:.1f}%\")\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\n# Bar chart by MAE\ncolors = ['green' if t == 'Advanced' else 'steelblue' for t in all_results['Type']]\nchampion_idx = all_results[all_results['Model'] == champion_model_name].index[0]\ncolors[list(all_results.index).index(champion_idx)] = 'gold'\n\naxes[0].barh(all_results['Model'], all_results['MAE'], color=colors)\naxes[0].set_xlabel('MAE (PLN)')\naxes[0].set_title('Model Comparison by MAE')\naxes[0].invert_yaxis()\n\n# Time series plot with champion prediction\naxes[1].plot(test.index, y_test.values, 'k-', linewidth=2, label='Actual', alpha=0.8)\nif champion_model_name == 'XGBoost (tuned)':\n    champion_pred = best_xgb_pred\nelif champion_model_name == 'XGBoost (default)':\n    champion_pred = xgb_pred\nelif champion_model_name == 'Prophet':\n    champion_pred = prophet_pred\nelse:\n    champion_pred = best_xgb_pred  # fallback\n    \naxes[1].plot(test.index, champion_pred, 'g--', linewidth=2, label=f'{champion_model_name} (Champion)', alpha=0.8)\naxes[1].set_title('Champion Model: Actual vs Predicted')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Sales (PLN)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9mbbr28vamq",
   "source": "## 9. MLflow Logging & Champion Model Selection\n\nLogging experiments to MLflow for reproducibility and tracking.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ozbjyrdc8",
   "source": "# ============================================\n# MLFLOW LOGGING\n# ============================================\n\n# Set up MLflow\nmlflow.set_experiment('Time_Series_Forecasting')\n\nprint(\"=\"*50)\nprint(\"LOGGING TO MLFLOW\")\nprint(\"=\"*50)\n\n# Log the tuned XGBoost model (likely champion)\nwith mlflow.start_run(run_name='XGBoost_Tuned_Champion'):\n    # Log parameters\n    mlflow.log_params(best_params)\n    mlflow.log_param('n_features', len(features))\n    mlflow.log_param('train_size', len(train))\n    mlflow.log_param('test_size', len(test))\n    \n    # Log metrics\n    mlflow.log_metric('mae', best_mae)\n    mlflow.log_metric('rmse', best_rmse)\n    mlflow.log_metric('mape', best_mape)\n    \n    if best_baseline_mae:\n        mlflow.log_metric('improvement_over_baseline', improvement)\n    \n    # Log model\n    mlflow.sklearn.log_model(best_xgb, 'model')\n    \n    # Log feature importance as artifact\n    importance_df.to_csv(RESULTS_DIR / 'feature_importance.csv', index=False)\n    mlflow.log_artifact(str(RESULTS_DIR / 'feature_importance.csv'))\n    \n    run_id = mlflow.active_run().info.run_id\n    print(f\"‚úÖ Logged XGBoost (tuned) to MLflow\")\n    print(f\"   Run ID: {run_id}\")\n\n# Log Prophet model\nwith mlflow.start_run(run_name='Prophet'):\n    mlflow.log_param('yearly_seasonality', True)\n    mlflow.log_param('weekly_seasonality', True)\n    mlflow.log_metric('mae', prophet_mae)\n    mlflow.log_metric('rmse', prophet_rmse)\n    mlflow.log_metric('mape', prophet_mape)\n    print(f\"‚úÖ Logged Prophet to MLflow\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a0b4l0tf7uq",
   "source": "## 10. Save Champion Model\n\nSaving the champion model for production deployment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "auzmgxn1u5",
   "source": "# ============================================\n# SAVE CHAMPION MODEL\n# ============================================\n\n# Save XGBoost tuned model (champion)\nchampion_model_path = MODELS_DIR / \"champion_model.pkl\"\njoblib.dump(best_xgb, champion_model_path)\nprint(f\"‚úÖ Saved champion model to: {champion_model_path}\")\n\n# Save model metadata\nmodel_metadata = {\n    'model_name': 'XGBoost (tuned)',\n    'best_params': best_params,\n    'features': features,\n    'metrics': {\n        'mae': best_mae,\n        'rmse': best_rmse,\n        'mape': best_mape\n    },\n    'train_date_range': {\n        'start': str(train.index.min().date()),\n        'end': str(train.index.max().date())\n    },\n    'test_date_range': {\n        'start': str(test.index.min().date()),\n        'end': str(test.index.max().date())\n    }\n}\n\nimport json\nmetadata_path = MODELS_DIR / \"champion_model_metadata.json\"\nwith open(metadata_path, 'w') as f:\n    json.dump(model_metadata, f, indent=2)\nprint(f\"‚úÖ Saved model metadata to: {metadata_path}\")\n\n# Save all results\nall_results.to_csv(RESULTS_DIR / 'all_model_results.csv', index=False)\nprint(f\"‚úÖ Saved all results to: {RESULTS_DIR / 'all_model_results.csv'}\")\n\n# Save anomaly data\nanomaly_cols = ['sales', 'anomaly_isoforest', 'anomaly_zscore', 'anomaly_iqr', 'anomaly_any']\ndf[anomaly_cols].to_parquet(PROCESSED_DIR / 'anomalies_detected.parquet')\nprint(f\"‚úÖ Saved anomaly data to: {PROCESSED_DIR / 'anomalies_detected.parquet'}\")\n\n# Final summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 3 COMPLETE - SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\"\"\nüìä ANOMALY DETECTION\n   - Isolation Forest: {df['anomaly_isoforest'].sum()} anomalies\n   - Z-score: {df['anomaly_zscore'].sum()} anomalies\n   - IQR: {df['anomaly_iqr'].sum()} anomalies\n\nüîß FEATURE ENGINEERING\n   - {len(features)} features created\n   - Top features: {', '.join(importance_df.head(5)['feature'].tolist())}\n\nüèÜ CHAMPION MODEL: XGBoost (tuned)\n   - MAE: {best_mae:,.0f} PLN\n   - RMSE: {best_rmse:,.0f} PLN\n   - MAPE: {best_mape:.1f}%\n\"\"\")\n\nif best_baseline_mae:\n    print(f\"\"\"üìà IMPROVEMENT OVER BASELINE\n   - Best baseline MAE: {best_baseline_mae:,.0f} PLN\n   - Champion MAE: {best_mae:,.0f} PLN\n   - Improvement: {improvement:.1f}%\n\"\"\")\n\nprint(\"\"\"\nüìÅ SAVED ARTIFACTS\n   - models/champion_model.pkl\n   - models/champion_model_metadata.json\n   - results/all_model_results.csv\n   - results/feature_importance.csv\n   - Data/processed/anomalies_detected.parquet\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}